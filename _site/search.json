[
  {
    "objectID": "README.html",
    "href": "README.html",
    "title": "This is my Consulting website if you like my blogs and wanna work with me feel free to schedule call from the above link!!",
    "section": "",
    "text": "This is my Consulting website if you like my blogs and wanna work with me feel free to schedule call from the above link!!"
  },
  {
    "objectID": "blog/Deep-RL/index.html",
    "href": "blog/Deep-RL/index.html",
    "title": "CPU Scheduling with Deep Reinforcement Learning",
    "section": "",
    "text": "CPU Scheduling with Deep Reinforcement Learning\n  \n  PPO Architecture\n  \n  Future Rewards Calculation\n  \n  Code Implementation\n  \n  Advantage Function\n  \n  Advantage Calculation\n  Code Implementation\n  \n  Clipped Surrogate Loss\n  \n  Key Components\n  \n  Policy Loss\n  \n  Explanation\n  \n  Loss Optimization\n  \n  Neural Network Architecture\n  Custom Gym Environment (PrioritySchedulerEnv)\n  Observation Space\n  Action Space\n  Reward Function\n  Code: Reward Function\n  Priority Queue Structure\n  Step Function\n  Code: Step Method\n  Multivariate Normal Distribution\n  \n  Formula\n  \n  Code: Action Sampling\n  Environment Initialization\n  Result"
  },
  {
    "objectID": "blog/Deep-RL/index.html#cpu-scheduling-with-deep-reinforcement-learning",
    "href": "blog/Deep-RL/index.html#cpu-scheduling-with-deep-reinforcement-learning",
    "title": "CPU Scheduling with Deep Reinforcement Learning",
    "section": "CPU Scheduling with Deep Reinforcement Learning",
    "text": "CPU Scheduling with Deep Reinforcement Learning\nThis project develops a Deep Reinforcement Learning (DRL) model to optimize CPU scheduling, aiming to reduce turnaround time and outperform the traditional Round-Robin algorithm. The model uses Proximal Policy Optimization (PPO) to dynamically adjust task priorities for enhanced scheduling efficiency.\nPlz refer to the code for a complete picture here. I have only explained important concepts!\n\nPPO Architecture\n\nPolicy Network\n\nInput: Current state \\(s_t\\)\nOutput: Action probabilities \\(pi(a_t / s_t)\\)\n\n\n\nValue Network\n\nInput: Current state \\(s_t\\) \\(a_t\\)\nOutput: Value estimate \\(V(s_t)\\)\n\n\n\nData Store\n\nThe agent processes the environment through the policy network, producing a probability distribution over actions.\nSelected actions, states, rewards, and probabilities, are stored in a data store for every batch."
  },
  {
    "objectID": "blog/Deep-RL/index.html#future-rewards-calculation",
    "href": "blog/Deep-RL/index.html#future-rewards-calculation",
    "title": "CPU Scheduling with Deep Reinforcement Learning",
    "section": "Future Rewards Calculation",
    "text": "Future Rewards Calculation\nFuture rewards, also known as Return-to-Go are computed recursively as: \\(G*t = r_t + \\gamma G*{t+1}\\)\nThis is implemented using the cumulative discounted reward formula:\ndiscounted_reward = rew + (discounted_reward * self.gamma)\nExpanded Formula: \\(G*t = r_t + \\gamma r*{t+1} + \\gamma^2 r\\_{t+2} + \\dots\\)\n\nCode Implementation\ndef compute_rtgs(self, batch_rews):\n    batch_rtgs = []\n    for ep_rews in reversed(batch_rews):\n        discounted_reward = 0\n        for rew in reversed(ep_rews):\n            discounted_reward = rew + (discounted_reward * self.gamma)\n            batch_rtgs.insert(0, discounted_reward)\n    return torch.tensor(batch_rtgs, dtype=torch.float)"
  },
  {
    "objectID": "blog/Deep-RL/index.html#advantage-function",
    "href": "blog/Deep-RL/index.html#advantage-function",
    "title": "CPU Scheduling with Deep Reinforcement Learning",
    "section": "Advantage Function",
    "text": "Advantage Function\nThe advantage function measures how much better an action \\(a_t\\) is compared to the expected policy behavior: \\(A_t = G_t - V(s_t)\\)\n\n\\(G_t\\): Future rewards, computed in the compute_rtgs method.\n\\(V(s_t)\\): Value estimate obtained from the critic network.\n\n\nAdvantage Calculation\nAdvantage is computed in the PPO learn method:\n# Calculate Advantage\nA_k = batch_rtgs - V.detach()\n\nbatch_rtgs: Precomputed future rewards using compute_rtgs.\nV.detach(): Ensures the critic networkâ€™s gradients do not interfere with the calculation.\n\n\n\nCode Implementation\n# todo Generalized Advantage Estimation GAE is a method to estimate the advantage function, which represents how much better an action is compared to the average action in a given state\nV, _ = self.evaluate(batch_obs, batch_acts)\n\ndef evaluate(self, batch_obs, batch_acts):\n        # query critic network for value V for each obs in batch_obs after encoding\n        # batch_obs = self.obs_enc(batch_obs)\n        V = self.critic(batch_obs).squeeze()\n        # print('eval', V.detach().shape)\n\n        # get log probabilities\n        mean = self.actor(batch_obs)\n        dist = MultivariateNormal(mean, self.cov_mat)\n        log_probs = dist.log_prob(batch_acts)\n        return V, log_probs"
  },
  {
    "objectID": "blog/Deep-RL/index.html#clipped-surrogate-loss",
    "href": "blog/Deep-RL/index.html#clipped-surrogate-loss",
    "title": "CPU Scheduling with Deep Reinforcement Learning",
    "section": "Clipped Surrogate Loss",
    "text": "Clipped Surrogate Loss\nThe clipped surrogate loss ensures stable policy updates by constraining deviations from the previous policy.\n\nKey Components\nProbability Ratio \\(r_t(\\theta)\\): \\[r*t(\\theta) = \\frac{\\pi*\\theta(a*t | s_t)}{\\pi*{\\theta\\_{\\text{old}}}(a_t | s_t)}\\]\nClipping Constraint: \\[\\text{clip}\\left( r_t(\\theta), 1-\\epsilon, 1+\\epsilon \\right)\\]\nObjective Function: \\[\\mathcal{L}^\\text{CLIP} = \\mathbb{E}\\left[ \\min\\left(r_t(\\theta) A_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) A_t\\right) \\right]\\]"
  },
  {
    "objectID": "blog/Deep-RL/index.html#policy-loss",
    "href": "blog/Deep-RL/index.html#policy-loss",
    "title": "CPU Scheduling with Deep Reinforcement Learning",
    "section": "Policy Loss",
    "text": "Policy Loss\nThe actor network uses the clipped surrogate loss for optimization:\n# Compute probability ratios\nlog_probs = self.actor(batch_obs).log_prob(batch_actions)\nold_log_probs = batch_log_probs.detach()\nratios = torch.exp(log_probs - old_log_probs)\n\n# Clipped surrogate loss\nsurr1 = ratios * batch_advantages\nsurr2 = torch.clamp(ratios, 1 - self.clip, 1 + self.clip) * batch_advantages\nactor_loss = -torch.mean(torch.min(surr1, surr2))\n\n# Backpropagate and optimize\nself.actor_optimizer.zero_grad()\nactor_loss.backward()\nself.actor_optimizer.step()\n\nExplanation\nLog-Probabilities:\n\nCurrent policy: \\[\\log \\pi\\_\\theta(a_t | s_t)\\]\nOld policy: \\[\\log \\pi*{\\theta*{\\text{old}}}(a_t | s_t)\\]\n\nProbability Ratios: \\[r*t(\\theta) = \\exp(\\log \\pi*\\theta - \\log \\pi*{\\theta*{\\text{old}}})\\]\nSurrogate Objectives:\n\nOriginal objective: \\[r_t(\\theta) A_t\\]\nClipped objective: \\[\\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) A_t\\]\n\nLoss Minimization:\n\nThe minimum of these ensures stability during training: \\[\\text{Loss} = -\\mathbb{E}\\left[\\min\\left(r_t(\\theta) A_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) A_t\\right)\\right]\\]"
  },
  {
    "objectID": "blog/Deep-RL/index.html#loss-optimization",
    "href": "blog/Deep-RL/index.html#loss-optimization",
    "title": "CPU Scheduling with Deep Reinforcement Learning",
    "section": "Loss Optimization",
    "text": "Loss Optimization\nThe actor network updates using the calculated loss:\nself.actor_optimizer.zero_grad()\nactor_loss.backward()\nself.actor_optimizer.step()\nThe critic network is trained similarly, minimizing the squared error between \\(G_t\\) and \\(V(s_t)\\).\n\n\nNeural Network Architecture\nThe implementation uses a feed-forward neural network with the following structure:\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport numpy as np\n\nclass FeedForwardNN(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(FeedForwardNN, self).__init__()\n\n        self.layer1 = nn.Linear(in_dim, 64)\n        self.layer2 = nn.Linear(64, 64)\n        self.layer3 = nn.Linear(64, out_dim)\n\n    def forward(self, obs):\n        if isinstance(obs, np.ndarray):\n            obs = torch.tensor(obs, dtype=torch.float)\n\n        # todo applies relu activation to the output of the layer\n        activation1 = F.relu(self.layer1(obs))\n        activation2 = F.relu(self.layer2(activation1))\n        output = self.layer3(activation2)\n\n        return output\n\n\nCustom Gym Environment (PrioritySchedulerEnv)\nThe custom environment models a priority-based process scheduler where processes are dynamically managed based on their arrival time, instruction count, and assigned priority. This environment provides structured data for PPO to learn effective scheduling strategies.\n\n\n\nObservation Space\nThe observation space is a matrix of shape (encoder_context + 1, 5). It provides the agent with state information, including:\n\nCurrent process: \\(\\text{PID}, \\text{arrival time}, \\text{total instructions}, \\text{remaining instructions}\\)\nPriority queue state for the current scheduling context.\n\n\nCode\nimport gymnasium as gym\nimport numpy as np\n\nclass PrioritySchedulerEnv(gym.Env):\n    def __init__(self, encoder_context=5, max_priority=10):\n        super().__init__()\n        self.encoder_context = encoder_context\n        self.max_priority = max_priority\n\n        # Observation space: (encoder_context + 1 processes, 5 features each)\n        self.observation_space = gym.spaces.Box(\n            low=0,\n            high=np.inf,\n            shape=(encoder_context + 1, 5),\n            dtype=np.float32\n        )\n\n        # Action space: Assign priority levels (0 to max_priority-1)\n        self.action_space = gym.spaces.Discrete(max_priority)\n\n        self.reset()\n\n\n\n\nAction Space\nThe action space is discrete, representing the priority levels that can be assigned to processes. The range is \\((0)\\) to {max_priority} - 1.\n\n\n\nReward Function\nThe reward function balances high throughput and low turnaround times:\n\nPositive Reward: \\(100 \\times \\text{completed processes}\\)\nPenalty: \\(-\\text{sum(turnaround times)}\\)\n\n\nFormula: Reward Function\n\\[\n\\text{Reward} = 100 \\times \\text{len(completed processes)} - \\sum(\\text{turnaround times})\n\\]\nThis encourages the agent to:\n\nMaximize completed processes.\nMinimize the turnaround time for each process.\n\n\n\n\nCode: Reward Function\ndef calculate_reward(self):\n    throughput_bonus = 100 * len(self.completed_processes)\n    latency_penalty = -sum(p[1] for p in self.completed_processes)  # Sum of turnaround times\n    return throughput_bonus + latency_penalty\n\nExample Scenarios\n\nFast Completion:\n\nCompleted Processes: [(0, 5), (1, 6)]\nReward: \\[200 - (5 + 6) = 189\\]\n\nSlow Completion:\n\nCompleted Processes: [(0, 15), (1, 18)]\nReward: \\[200 - (15 + 18) = 167\\]\n\n\n\n\n\n\nPriority Queue Structure\nProcesses are managed in a priority queue:\\[{priority}, [\\text{PID}, \\text{arrival time}, \\text{total instructions}, \\text{remaining instructions}]\\]\nself.execution_queue.put(\n    (assign_priority, (self.processes[self.data_pointer]))\n)\n\nassign_priority: This value is determined based on the output of the PPO algorithm (using the provided action). self.processes[self.data_pointer]: Contains the process details such as PID, arrival time, instructions, and remaining instructions. Sorting by Priority: The PriorityQueue ensures processes are sorted automatically based on priority, with lower values representing higher priority.\nself.processes[self.data_pointer]: Contains the process details such as PID, arrival time, instructions, and remaining instructions.\nThe PriorityQueue ensures processes are sorted automatically based on priority, with lower values representing higher priority\n\n\nExample Queue\n(2, [0, 1, 10, 8])  # Priority 2, PID 0, 8 instructions remaining\n(3, [2, 7, 12, 12]) # Priority 3, PID 2, 12 instructions remaining\n(5, [1, 3, 8, 8])   # Priority 5, PID 1, 8 instructions remaining\n\n\n\n\nStep Function\nThe step method handles environment dynamics:\n\nUpdates process states based on the selected action (priority assignment).\nManages process execution and computes rewards.\n\n\n\nCode: Step Method\ndef step(self, action):\n    # Update priority of the current process\n    process = self.current_process\n    process_priority = action\n    self.add_to_queue(process, process_priority)\n\n    # Execute the highest-priority process\n    next_process = self.get_next_process()\n    if next_process:\n        _, process_data = next_process\n        process_data[3] -= 1  # Decrement remaining instructions\n\n        if process_data[3] == 0:  # Process completed\n            self.completed_processes.append((process_data[0], self.current_time - process_data[1]))\n\n    # Increment time and compute reward\n    self.current_time += 1\n    reward = self.calculate_reward()\n\n    # Update observation (current process + priority queue context)\n    observation = self.get_observation()\n    done = len(self.priority_queue) == 0\n    return observation, reward, done, {}\n\n\n\nMultivariate Normal Distribution\nPPO uses a multivariate normal distribution for action selection, ensuring diverse priority assignments while preserving stability."
  },
  {
    "objectID": "blog/Deep-RL/index.html#formula",
    "href": "blog/Deep-RL/index.html#formula",
    "title": "CPU Scheduling with Deep Reinforcement Learning",
    "section": "Formula",
    "text": "Formula\n\\[\n\\pi(a_t | s_t) = \\frac{1}{\\sqrt{(2\\pi)^k |\\Sigma|}} \\exp\\left(-\\frac{1}{2}(a_t - \\mu)^T \\Sigma^{-1} (a_t - \\mu)\\right)\n\\]\nWhere:\n\n\\(mu\\): Mean vector (policyâ€™s output).\n\\(Sigma\\): Covariance matrix.\n\n\nCode: Action Sampling\n    def get_action(self, obs):\n        # encode the observations and query the actor for mean action\n        # obs = self.obs_enc(obs)\n        mean = self.actor(obs)\n\n        # create multivariate normal distribution\n        dist = MultivariateNormal(mean, self.cov_mat)\n\n        # sample action from distribution\n        action = dist.sample()\n        log_prob = dist.log_prob(action)\n\n        # return detached action and log prob\n        return action.detach().numpy(), log_prob.detach().numpy()\n\n\nEnvironment Initialization\nThe environment initializes with the following:\n\nProcesses: A batch of random processes with arrival times and instruction counts.\nPriority Queue: Starts empty, populated dynamically during runtime.\n\n\nCode: Initialization\ndef reset(self):\n    self.priority_queue = []\n    self.completed_processes = []\n    self.current_time = 0\n\n    # Generate random processes\n    self.processes = [\n        [i, np.random.randint(0, 10), np.random.randint(5, 15), np.random.randint(5, 15)]\n        for i in range(self.encoder_context + 1)\n    ]\n    self.current_process = self.processes.pop(0)\n    return self.get_observation()\n\n\n\n\nResult\nThis way we can Train a Deep-RL Model to outperform classic algorithms like round robin.\n\n\n\nStats"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html#work-with-me",
    "href": "index.html#work-with-me",
    "title": "Aditya's Blog",
    "section": "ðŸ’¼ Work With Me",
    "text": "ðŸ’¼ Work With Me\nDo you need help operationalizing AI and large language models? Iâ€™m open to new consulting work and other forms of advisory. If you need help with your project lets chat Calendly or Email me"
  },
  {
    "objectID": "index.html#feed",
    "href": "index.html#feed",
    "title": "Aditya's Blog",
    "section": "ðŸ“® Feed",
    "text": "ðŸ“® Feed\nI often share my experience building AI products. Below is a selected assortment of my longer-form writing on my machine learning work.\n\n\n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\n11/7/24\n\n\nCPU Scheduling with Deep Reinforcement Learning\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#follow-me",
    "href": "index.html#follow-me",
    "title": "Aditya's Blog",
    "section": "ðŸ“¬ Follow Me",
    "text": "ðŸ“¬ Follow Me\nYou can subscribe to my blog via the button below. Iâ€™m also active on Twitter."
  }
]