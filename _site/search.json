[
  {
    "objectID": "README.html",
    "href": "README.html",
    "title": "This is my Consulting website if you like my blogs and wanna work with me feel free to schedule call from the above link!!",
    "section": "",
    "text": "This is my Consulting website if you like my blogs and wanna work with me feel free to schedule call from the above link!!"
  },
  {
    "objectID": "blog/Deep-RL/index.html",
    "href": "blog/Deep-RL/index.html",
    "title": "CPU Scheduling with Deep Reinforcement Learning",
    "section": "",
    "text": "CPU Scheduling with Deep Reinforcement Learning\n  \n  PPO Architecture\n  \n  Future Rewards Calculation\n  \n  Code Implementation\n  \n  Advantage Function\n  \n  Advantage Calculation\n  Code Implementation\n  \n  Clipped Surrogate Loss\n  \n  Key Components\n  \n  Policy Loss\n  \n  Explanation\n  \n  Loss Optimization\n  \n  Neural Network Architecture\n  Custom Gym Environment (PrioritySchedulerEnv)\n  Observation Space\n  Action Space\n  Reward Function\n  Code: Reward Function\n  Priority Queue Structure\n  Step Function\n  Code: Step Method\n  Multivariate Normal Distribution\n  \n  Formula\n  \n  Code: Action Sampling\n  Environment Initialization\n  Result"
  },
  {
    "objectID": "blog/Deep-RL/index.html#cpu-scheduling-with-deep-reinforcement-learning",
    "href": "blog/Deep-RL/index.html#cpu-scheduling-with-deep-reinforcement-learning",
    "title": "CPU Scheduling with Deep Reinforcement Learning",
    "section": "CPU Scheduling with Deep Reinforcement Learning",
    "text": "CPU Scheduling with Deep Reinforcement Learning\nThis project develops a Deep Reinforcement Learning (DRL) model to optimize CPU scheduling, aiming to reduce turnaround time and outperform the traditional Round-Robin algorithm. The model uses Proximal Policy Optimization (PPO) to dynamically adjust task priorities for enhanced scheduling efficiency.\nPlz refer to the code for a complete picture here. I have only explained important concepts!\n\nPPO Architecture\n\nPolicy Network\n\nInput: Current state \\(s_t\\)\nOutput: Action probabilities \\(pi(a_t / s_t)\\)\n\n\n\nValue Network\n\nInput: Current state \\(s_t\\) \\(a_t\\)\nOutput: Value estimate \\(V(s_t)\\)\n\n\n\nData Store\n\nThe agent processes the environment through the policy network, producing a probability distribution over actions.\nSelected actions, states, rewards, and probabilities, are stored in a data store for every batch."
  },
  {
    "objectID": "blog/Deep-RL/index.html#future-rewards-calculation",
    "href": "blog/Deep-RL/index.html#future-rewards-calculation",
    "title": "CPU Scheduling with Deep Reinforcement Learning",
    "section": "Future Rewards Calculation",
    "text": "Future Rewards Calculation\nFuture rewards, also known as Return-to-Go are computed recursively as: \\(G*t = r_t + \\gamma G*{t+1}\\)\nThis is implemented using the cumulative discounted reward formula:\ndiscounted_reward = rew + (discounted_reward * self.gamma)\nExpanded Formula: \\(G*t = r_t + \\gamma r*{t+1} + \\gamma^2 r\\_{t+2} + \\dots\\)\n\nCode Implementation\ndef compute_rtgs(self, batch_rews):\n    batch_rtgs = []\n    for ep_rews in reversed(batch_rews):\n        discounted_reward = 0\n        for rew in reversed(ep_rews):\n            discounted_reward = rew + (discounted_reward * self.gamma)\n            batch_rtgs.insert(0, discounted_reward)\n    return torch.tensor(batch_rtgs, dtype=torch.float)"
  },
  {
    "objectID": "blog/Deep-RL/index.html#advantage-function",
    "href": "blog/Deep-RL/index.html#advantage-function",
    "title": "CPU Scheduling with Deep Reinforcement Learning",
    "section": "Advantage Function",
    "text": "Advantage Function\nThe advantage function measures how much better an action \\(a_t\\) is compared to the expected policy behavior: \\(A_t = G_t - V(s_t)\\)\n\n\\(G_t\\): Future rewards, computed in the compute_rtgs method.\n\\(V(s_t)\\): Value estimate obtained from the critic network.\n\n\nAdvantage Calculation\nAdvantage is computed in the PPO learn method:\n# Calculate Advantage\nA_k = batch_rtgs - V.detach()\n\nbatch_rtgs: Precomputed future rewards using compute_rtgs.\nV.detach(): Ensures the critic network‚Äôs gradients do not interfere with the calculation.\n\n\n\nCode Implementation\n# todo Generalized Advantage Estimation GAE is a method to estimate the advantage function, which represents how much better an action is compared to the average action in a given state\nV, _ = self.evaluate(batch_obs, batch_acts)\n\ndef evaluate(self, batch_obs, batch_acts):\n        # query critic network for value V for each obs in batch_obs after encoding\n        # batch_obs = self.obs_enc(batch_obs)\n        V = self.critic(batch_obs).squeeze()\n        # print('eval', V.detach().shape)\n\n        # get log probabilities\n        mean = self.actor(batch_obs)\n        dist = MultivariateNormal(mean, self.cov_mat)\n        log_probs = dist.log_prob(batch_acts)\n        return V, log_probs"
  },
  {
    "objectID": "blog/Deep-RL/index.html#clipped-surrogate-loss",
    "href": "blog/Deep-RL/index.html#clipped-surrogate-loss",
    "title": "CPU Scheduling with Deep Reinforcement Learning",
    "section": "Clipped Surrogate Loss",
    "text": "Clipped Surrogate Loss\nThe clipped surrogate loss ensures stable policy updates by constraining deviations from the previous policy.\n\nKey Components\nProbability Ratio \\(r_t(\\theta)\\): \\[r*t(\\theta) = \\frac{\\pi*\\theta(a*t | s_t)}{\\pi*{\\theta\\_{\\text{old}}}(a_t | s_t)}\\]\nClipping Constraint: \\[\\text{clip}\\left( r_t(\\theta), 1-\\epsilon, 1+\\epsilon \\right)\\]\nObjective Function: \\[\\mathcal{L}^\\text{CLIP} = \\mathbb{E}\\left[ \\min\\left(r_t(\\theta) A_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) A_t\\right) \\right]\\]"
  },
  {
    "objectID": "blog/Deep-RL/index.html#policy-loss",
    "href": "blog/Deep-RL/index.html#policy-loss",
    "title": "CPU Scheduling with Deep Reinforcement Learning",
    "section": "Policy Loss",
    "text": "Policy Loss\nThe actor network uses the clipped surrogate loss for optimization:\n# Compute probability ratios\nlog_probs = self.actor(batch_obs).log_prob(batch_actions)\nold_log_probs = batch_log_probs.detach()\nratios = torch.exp(log_probs - old_log_probs)\n\n# Clipped surrogate loss\nsurr1 = ratios * batch_advantages\nsurr2 = torch.clamp(ratios, 1 - self.clip, 1 + self.clip) * batch_advantages\nactor_loss = -torch.mean(torch.min(surr1, surr2))\n\n# Backpropagate and optimize\nself.actor_optimizer.zero_grad()\nactor_loss.backward()\nself.actor_optimizer.step()\n\nExplanation\nLog-Probabilities:\n\nCurrent policy: \\[\\log \\pi\\_\\theta(a_t | s_t)\\]\nOld policy: \\[\\log \\pi*{\\theta*{\\text{old}}}(a_t | s_t)\\]\n\nProbability Ratios: \\[r*t(\\theta) = \\exp(\\log \\pi*\\theta - \\log \\pi*{\\theta*{\\text{old}}})\\]\nSurrogate Objectives:\n\nOriginal objective: \\[r_t(\\theta) A_t\\]\nClipped objective: \\[\\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) A_t\\]\n\nLoss Minimization:\n\nThe minimum of these ensures stability during training: \\[\\text{Loss} = -\\mathbb{E}\\left[\\min\\left(r_t(\\theta) A_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) A_t\\right)\\right]\\]"
  },
  {
    "objectID": "blog/Deep-RL/index.html#loss-optimization",
    "href": "blog/Deep-RL/index.html#loss-optimization",
    "title": "CPU Scheduling with Deep Reinforcement Learning",
    "section": "Loss Optimization",
    "text": "Loss Optimization\nThe actor network updates using the calculated loss:\nself.actor_optimizer.zero_grad()\nactor_loss.backward()\nself.actor_optimizer.step()\nThe critic network is trained similarly, minimizing the squared error between \\(G_t\\) and \\(V(s_t)\\).\n\n\nNeural Network Architecture\nThe implementation uses a feed-forward neural network with the following structure:\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport numpy as np\n\nclass FeedForwardNN(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(FeedForwardNN, self).__init__()\n\n        self.layer1 = nn.Linear(in_dim, 64)\n        self.layer2 = nn.Linear(64, 64)\n        self.layer3 = nn.Linear(64, out_dim)\n\n    def forward(self, obs):\n        if isinstance(obs, np.ndarray):\n            obs = torch.tensor(obs, dtype=torch.float)\n\n        # todo applies relu activation to the output of the layer\n        activation1 = F.relu(self.layer1(obs))\n        activation2 = F.relu(self.layer2(activation1))\n        output = self.layer3(activation2)\n\n        return output\n\n\nCustom Gym Environment (PrioritySchedulerEnv)\nThe custom environment models a priority-based process scheduler where processes are dynamically managed based on their arrival time, instruction count, and assigned priority. This environment provides structured data for PPO to learn effective scheduling strategies.\n\n\n\nObservation Space\nThe observation space is a matrix of shape (encoder_context + 1, 5). It provides the agent with state information, including:\n\nCurrent process: \\(\\text{PID}, \\text{arrival time}, \\text{total instructions}, \\text{remaining instructions}\\)\nPriority queue state for the current scheduling context.\n\n\nCode\nimport gymnasium as gym\nimport numpy as np\n\nclass PrioritySchedulerEnv(gym.Env):\n    def __init__(self, encoder_context=5, max_priority=10):\n        super().__init__()\n        self.encoder_context = encoder_context\n        self.max_priority = max_priority\n\n        # Observation space: (encoder_context + 1 processes, 5 features each)\n        self.observation_space = gym.spaces.Box(\n            low=0,\n            high=np.inf,\n            shape=(encoder_context + 1, 5),\n            dtype=np.float32\n        )\n\n        # Action space: Assign priority levels (0 to max_priority-1)\n        self.action_space = gym.spaces.Discrete(max_priority)\n\n        self.reset()\n\n\n\n\nAction Space\nThe action space is discrete, representing the priority levels that can be assigned to processes. The range is \\((0)\\) to {max_priority} - 1.\n\n\n\nReward Function\nThe reward function balances high throughput and low turnaround times:\n\nPositive Reward: \\(100 \\times \\text{completed processes}\\)\nPenalty: \\(-\\text{sum(turnaround times)}\\)\n\n\nFormula: Reward Function\n\\[\n\\text{Reward} = 100 \\times \\text{len(completed processes)} - \\sum(\\text{turnaround times})\n\\]\nThis encourages the agent to:\n\nMaximize completed processes.\nMinimize the turnaround time for each process.\n\n\n\n\nCode: Reward Function\ndef calculate_reward(self):\n    throughput_bonus = 100 * len(self.completed_processes)\n    latency_penalty = -sum(p[1] for p in self.completed_processes)  # Sum of turnaround times\n    return throughput_bonus + latency_penalty\n\nExample Scenarios\n\nFast Completion:\n\nCompleted Processes: [(0, 5), (1, 6)]\nReward: \\[200 - (5 + 6) = 189\\]\n\nSlow Completion:\n\nCompleted Processes: [(0, 15), (1, 18)]\nReward: \\[200 - (15 + 18) = 167\\]\n\n\n\n\n\n\nPriority Queue Structure\nProcesses are managed in a priority queue:\\[{priority}, [\\text{PID}, \\text{arrival time}, \\text{total instructions}, \\text{remaining instructions}]\\]\nself.execution_queue.put(\n    (assign_priority, (self.processes[self.data_pointer]))\n)\n\nassign_priority: This value is determined based on the output of the PPO algorithm (using the provided action). self.processes[self.data_pointer]: Contains the process details such as PID, arrival time, instructions, and remaining instructions. Sorting by Priority: The PriorityQueue ensures processes are sorted automatically based on priority, with lower values representing higher priority.\nself.processes[self.data_pointer]: Contains the process details such as PID, arrival time, instructions, and remaining instructions.\nThe PriorityQueue ensures processes are sorted automatically based on priority, with lower values representing higher priority\n\n\nExample Queue\n(2, [0, 1, 10, 8])  # Priority 2, PID 0, 8 instructions remaining\n(3, [2, 7, 12, 12]) # Priority 3, PID 2, 12 instructions remaining\n(5, [1, 3, 8, 8])   # Priority 5, PID 1, 8 instructions remaining\n\n\n\n\nStep Function\nThe step method handles environment dynamics:\n\nUpdates process states based on the selected action (priority assignment).\nManages process execution and computes rewards.\n\n\n\nCode: Step Method\ndef step(self, action):\n    # Update priority of the current process\n    process = self.current_process\n    process_priority = action\n    self.add_to_queue(process, process_priority)\n\n    # Execute the highest-priority process\n    next_process = self.get_next_process()\n    if next_process:\n        _, process_data = next_process\n        process_data[3] -= 1  # Decrement remaining instructions\n\n        if process_data[3] == 0:  # Process completed\n            self.completed_processes.append((process_data[0], self.current_time - process_data[1]))\n\n    # Increment time and compute reward\n    self.current_time += 1\n    reward = self.calculate_reward()\n\n    # Update observation (current process + priority queue context)\n    observation = self.get_observation()\n    done = len(self.priority_queue) == 0\n    return observation, reward, done, {}\n\n\n\nMultivariate Normal Distribution\nPPO uses a multivariate normal distribution for action selection, ensuring diverse priority assignments while preserving stability."
  },
  {
    "objectID": "blog/Deep-RL/index.html#formula",
    "href": "blog/Deep-RL/index.html#formula",
    "title": "CPU Scheduling with Deep Reinforcement Learning",
    "section": "Formula",
    "text": "Formula\n\\[\n\\pi(a_t | s_t) = \\frac{1}{\\sqrt{(2\\pi)^k |\\Sigma|}} \\exp\\left(-\\frac{1}{2}(a_t - \\mu)^T \\Sigma^{-1} (a_t - \\mu)\\right)\n\\]\nWhere:\n\n\\(mu\\): Mean vector (policy‚Äôs output).\n\\(Sigma\\): Covariance matrix.\n\n\nCode: Action Sampling\n    def get_action(self, obs):\n        # encode the observations and query the actor for mean action\n        # obs = self.obs_enc(obs)\n        mean = self.actor(obs)\n\n        # create multivariate normal distribution\n        dist = MultivariateNormal(mean, self.cov_mat)\n\n        # sample action from distribution\n        action = dist.sample()\n        log_prob = dist.log_prob(action)\n\n        # return detached action and log prob\n        return action.detach().numpy(), log_prob.detach().numpy()\n\n\nEnvironment Initialization\nThe environment initializes with the following:\n\nProcesses: A batch of random processes with arrival times and instruction counts.\nPriority Queue: Starts empty, populated dynamically during runtime.\n\n\nCode: Initialization\ndef reset(self):\n    self.priority_queue = []\n    self.completed_processes = []\n    self.current_time = 0\n\n    # Generate random processes\n    self.processes = [\n        [i, np.random.randint(0, 10), np.random.randint(5, 15), np.random.randint(5, 15)]\n        for i in range(self.encoder_context + 1)\n    ]\n    self.current_process = self.processes.pop(0)\n    return self.get_observation()\n\n\n\n\nResult\nThis way we can Train a Deep-RL Model to outperform classic algorithms like round robin.\n\n\n\nStats"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "blog/Deep-RL/hume.html",
    "href": "blog/Deep-RL/hume.html",
    "title": "Retrival Augmented Genration(RAG) with Hume-AI",
    "section": "",
    "text": "Retrival Augmented Genration(RAG) with Hume-AI\n  \n  Packages\n  Intialize Hume-AI\n  ClientComponent\n  Create Connection\n  Retrieve message content from the WS stream\n  Create Hume-Tool Caling with Schema\n  Qdrant-Vector Database\n  FileUpload\n  Handle File Upload on Frontend\n  ENJOY"
  },
  {
    "objectID": "blog/Deep-RL/hume.html#retrival-augmented-genrationrag-with-hume-ai",
    "href": "blog/Deep-RL/hume.html#retrival-augmented-genrationrag-with-hume-ai",
    "title": "Retrival Augmented Genration(RAG) with Hume-AI",
    "section": "Retrival Augmented Genration(RAG) with Hume-AI",
    "text": "Retrival Augmented Genration(RAG) with Hume-AI\nThis project attempts to implement Retrival Augmented Genration(RAG) with Function calling !!\nCheckout the complete code and live application for better understanding Live Github\n\nPackages\nnpm i hume\nnpm i @humeai/voice-react\nnpm i openai\nnpm i unstructured-client\nnpm i @qdrant/js-client-rest\n\n\nIntialize Hume-AI\nget access token and pass the token to VoiceProvider wrapper\n\"use server\";\nimport { fetchAccessToken } from \"hume\";\n\nconst HUME_SECRET_KEY = process.env.HUME_SECRET_KEY;\nconst HUME_API_KEY = process.env.HUME_API_KEY;\n\nexport default async function Page() {\n  const accessToken = await fetchAccessToken({\n    apiKey: HUME_API_KEY,\n    secretKey: HUME_SECRET_KEY,\n  });\n\n  return &lt;ClientComponent accessToken={accessToken} /&gt;;\n}\n\n\nClientComponent\nVoiceProvider creates a socket connection between the application and Hume-AI Wrapp the Front-end component like Chat-Box and start call button\n\"use client\";\n\nimport { VoiceProvider } from \"@humeai/voice-react\";\nimport { Cosine } from \"../Cosine\";\n\nexport default function ClientComponent({ accessToken }) {\n  return (\n    &lt;VoiceProvider\n      configId={process.env.NEXT_PUBLIC_HUME_CONFIG_ID}\n      auth={{ type: \"accessToken\", value: accessToken }}\n      onToolCall={handleToolCall}\n    &gt;\n      &lt;div&gt;\n        &lt;Messages /&gt;\n        &lt;Call /&gt;\n      &lt;/div&gt;\n    &lt;/VoiceProvider&gt;\n  );\n}\n\nconst handleToolCall = async (message, socket) =&gt; {\n  if (message.name === \"retrieve_data\") {\n    try {\n      const { query } = JSON.parse(message.parameters);\n\n      const data = await Cosine({ userText: query });\n\n      const toolResponseMessage = {\n        type: \"tool_response\",\n        toolCallId: message.toolCallId,\n        content: data,\n      };\n\n      return socket.success(toolResponseMessage);\n    } catch (error) {\n      return socket.error({\n        error: \"Embeddings retrieval error\",\n        code: 400,\n      });\n    }\n  }\n\n  return socket.error({\n    error: \"Tool not found\",\n    code: 401,\n  });\n};\n\n\nCreate Connection\nThis will create connection with the Hume-AI client with help of VoiceProvider wrapper\n\"use client\";\n\nimport { useVoice } from \"@humeai/voice-react\";\n\nexport function Call() {\n  const { connect, disconnect, status } = useVoice();\n\n  const handleClick = () =&gt; {\n    if (status.value === \"connected\") {\n      disconnect();\n    } else {\n      try {\n        connect();\n      } catch (error) {\n        console.log(\"Error connecting:\", error);\n      }\n    }\n  };\n\n  return (\n    &lt;div className=\"flex items-end justify-center\"&gt;\n      &lt;button disabled={status.value === \"connecting\"} onClick={handleClick}&gt;\n        {status.value === \"connected\" ? \"End Call\" : \"Start Call\"}\n      &lt;/button&gt;\n    &lt;/div&gt;\n  );\n}\n\n\nRetrieve message content from the WS stream\nDestructure the message content, role ‚Ä¶ from the useVoice !!\n\"use client\";\nimport { useVoice } from \"@humeai/voice-react\";\nimport { useEffect, useRef } from \"react\";\n\nexport default function Messages() {\n  const { messages } = useVoice();\n\n  return (\n    &lt;div&gt;\n      {messages.length === 0 && (\n        &lt;p&gt;Press Start Call to start the conversation With Prana-Bot!!&lt;/p&gt;\n      )}\n      {messages.map((msg, index) =&gt; {\n        if (msg.type !== \"user_message\" && msg.type !== \"assistant_message\")\n          return null;\n\n        const { role, content } = msg.message;\n        return (\n          &lt;div\n            key={msg.type + index}\n            className={`mb-1 ${\n              role === \"assistant\" ? \"justify-start\" : \"justify-end\"\n            } flex`}\n          &gt;\n            &lt;div\n              className={`chat-bubble max-w-[80%] break-words ${\n                role === \"assistant\"\n                  ? \"bg-base-200 text-primary\"\n                  : \"bg-primary text-white\"\n              }`}\n            &gt;\n              {content}\n            &lt;/div&gt;\n          &lt;/div&gt;\n        );\n      })}\n    &lt;/div&gt;\n  );\n}\n\n\nCreate Hume-Tool Caling with Schema\nRemember the name of tool to be same in handleToolCall function in my case its ‚Äúretrieve_data‚Äù\n\n\n\nschema\n\n\n{\n  \"type\": \"object\",\n  \"required\": [\"query\"],\n  \"properties\": {\n    \"query\": {\n      \"type\": \"string\",\n      \"description\": \"The search query to retrieve podcast data.\"\n    }\n  }\n}\n\n\nQdrant-Vector Database\nInitialize OpenAI QdrantClient to create embeddings and storing them in Qdrant DB\n\"use server\";\n\nimport { QdrantClient } from \"@qdrant/js-client-rest\";\nimport OpenAI from \"openai\";\n\nconst collection = \"test\";\n\nconst openai = new OpenAI({\n  apiKey: process.env.OPENAI_API_KEY,\n});\n\nconst client = new QdrantClient({\n  url: process.env.QDRANT_URL,\n  apiKey: process.env.QDRANT_API_KEY,\n});\n\n**Cosine-Similarity**\n\nexport async function Cosine(data) {\n  try {\n    const { userText } = data;\n\n    const embeddingResponse = await openai.embeddings.create({\n      model: \"text-embedding-3-large\",\n      input: userText,\n    });\n\n    const queryEmbedding = embeddingResponse.data[0].embedding;\n    const results = await client.search(collection, {\n      vector: queryEmbedding,\n      limit: 3,\n    });\n\n    const responseData = results.map(\n      (obj, i) =&gt; `${(i + 1).toString()}. ${obj.payload.page_content}`\n    );\n\n    return responseData.join(\"\\n\\n\");\n  } catch (error) {\n    return {\n      error: \"An error occurred during similarity search.\",\n    };\n  }\n}\n\n\nFileUpload\nParse the document and create Embeddings to Upsert them to Qdrant\n\"use server\";\n\nimport { promises as fs } from \"fs\";\nimport path from \"path\";\nimport { UnstructuredClient } from \"unstructured-client\";\nimport { Strategy } from \"unstructured-client/sdk/models/shared/index.js\";\nimport os from \"os\";\nimport { QdrantClient } from \"@qdrant/js-client-rest\";\nimport OpenAI from \"openai\";\n\nconst collectionName = \"test\";\nconst VECTOR_SIZE = 3072;\n\nconst unstructuredClient = new UnstructuredClient({\n  security: {\n    apiKeyAuth: \"*******\",\n  },\n});\n\nexport async function uploadFile(file) {\n  try {\n    await ensureCollectionExists();\n\n    const uploadDir = path.join(\n      // process.cwd(), \"src\", \"app\",\n      os.tmpdir(),\n      \"uploads\"\n    );\n    await fs.mkdir(uploadDir, { recursive: true });\n\n    const filePath = path.join(uploadDir, file.name);\n    await fs.writeFile(filePath, Buffer.from(await file.arrayBuffer()));\n\n    const fileData = await fs.readFile(filePath);\n\n    const response = await unstructuredClient.general.partition({\n      partitionParameters: {\n        files: {\n          content: fileData,\n          fileName: file.name,\n        },\n        strategy: Strategy.Auto,\n      },\n    });\n\n    const points = [];\n\n    for (const element of response.elements) {\n      try {\n        const embeddingResponse = await openai.embeddings.create({\n          model: \"text-embedding-3-large\",\n          input: element.text,\n        });\n\n        const embedding = embeddingResponse.data[0].embedding;\n\n        points.push({\n          id: crypto.randomUUID(),\n          vector: embedding,\n          payload: {\n            content: element.text,\n            metadata: {\n              type: element.type,\n              fileName: file.name,\n            },\n          },\n        });\n      } catch (error) {\n        console.error(\"Error creating embedding:\", error);\n      }\n    }\n\n    return {\n      success: true,\n      filePath,\n      message: `Successfully processed ${points.length} elements`,\n    };\n  } catch (error) {\n    return {\n      success: false,\n      error: error.message,\n    };\n  }\n}\n\n\nHandle File Upload on Frontend\n\"use client\";\nimport React, { useState } from \"react\";\nimport { uploadFile } from \"../UploadFile\";\n\nexport const File = () =&gt; {\n  const [message, setMessage] = useState(\"\");\n  const [selectedFile, setSelectedFile] = useState(null);\n  const [buttonText, setButtonText] = useState(\"Upload File\");\n\n  const handleFileChange = (event) =&gt; {\n    const file = event.target.files[0];\n    setSelectedFile(file);\n  };\n\n  const handleSubmit = async (event) =&gt; {\n    event.preventDefault();\n    if (selectedFile) {\n      setButtonText(\"Processing...\");\n      try {\n        const result = await uploadFile(selectedFile);\n        if (result.success) {\n          setMessage(`${result.filePath} - ${result.message}`);\n          setButtonText(\"Upload File\");\n        } else {\n          setMessage(\"File upload failed\");\n          setButtonText(\"Upload File\");\n        }\n      } catch (error) {\n        setMessage(`File upload failed: ${error.message}`);\n        setButtonText(\"Upload File\");\n      }\n    } else {\n      setMessage(\"No file selected\");\n    }\n  };\n\n  return (\n    &lt;div&gt;\n      &lt;form onSubmit={handleSubmit}&gt;\n        &lt;input type=\"file\" name=\"file\" onChange={handleFileChange} /&gt;\n        &lt;button type=\"submit\"&gt;{buttonText}&lt;/button&gt;\n        &lt;p&gt;{message}&lt;/p&gt;\n      &lt;/form&gt;\n    &lt;/div&gt;\n  );\n};\n\n\nENJOY"
  },
  {
    "objectID": "index.html#work-with-me",
    "href": "index.html#work-with-me",
    "title": "Aditya's Blog",
    "section": "üíº Work With Me",
    "text": "üíº Work With Me\nDo you need help operationalizing AI and large language models? I‚Äôm open to new consulting work and other forms of advisory. If you need help with your project lets chat Calendly or Email me"
  },
  {
    "objectID": "index.html#feed",
    "href": "index.html#feed",
    "title": "Aditya's Blog",
    "section": "üìÆ Feed",
    "text": "üìÆ Feed\nI often share my experience building AI products. Below is a selected assortment of my longer-form writing on my machine learning work.\n\n\n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\n11/20/24\n\n\nRetrival Augmented Genration(RAG) with Hume-AI\n\n\n\n\n11/7/24\n\n\nCPU Scheduling with Deep Reinforcement Learning\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#follow-me",
    "href": "index.html#follow-me",
    "title": "Aditya's Blog",
    "section": "üì¨ Follow Me",
    "text": "üì¨ Follow Me\nYou can subscribe to my blog via the button below. I‚Äôm also active on Twitter."
  }
]